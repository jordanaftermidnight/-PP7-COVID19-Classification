{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Chest X-Ray Classification\n",
    "\n",
    "This notebook implements a deep learning model to classify chest X-ray images as COVID-19 positive or negative.\n",
    "\n",
    "## Project Overview\n",
    "- **Objective**: Train a model to classify people as having COVID vs not having COVID based on chest X-ray images\n",
    "- **Target Accuracy**: >50% (better than random guessing)\n",
    "- **Dataset**: COVID-19 Radiography Database from Kaggle\n",
    "- **Framework**: PyTorch with transfer learning using ResNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the COVID-19 Radiography Database from Kaggle\n",
    "# For this demo, we'll create a simple structure and use a subset of data\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data/COVID', exist_ok=True)\n",
    "os.makedirs('data/Normal', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created!\")\n",
    "print(\"Please download the COVID-19 Radiography Database from:\")\n",
    "print(\"https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\")\n",
    "print(\"And extract the COVID and Normal folders to the data/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(covid_dir, normal_dir, max_samples_per_class=1000):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load COVID images (label = 1)\n",
    "    covid_files = [f for f in os.listdir(covid_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:max_samples_per_class]\n",
    "    for file in covid_files:\n",
    "        image_paths.append(os.path.join(covid_dir, file))\n",
    "        labels.append(1)\n",
    "    \n",
    "    # Load Normal images (label = 0)\n",
    "    normal_files = [f for f in os.listdir(normal_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:max_samples_per_class]\n",
    "    for file in normal_files:\n",
    "        image_paths.append(os.path.join(normal_dir, file))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# Data transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load data (this will work once the dataset is downloaded)\n",
    "try:\n",
    "    image_paths, labels = load_data('data/COVID', 'data/Normal')\n",
    "    print(f\"Loaded {len(image_paths)} images\")\n",
    "    print(f\"COVID cases: {sum(labels)}\")\n",
    "    print(f\"Normal cases: {len(labels) - sum(labels)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = COVID19Dataset(X_train, y_train, transform=transform_train)\n",
    "    test_dataset = COVID19Dataset(X_test, y_test, transform=transform_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please download and extract the dataset first.\")\n",
    "    print(\"For now, we'll create a mock dataset for demonstration.\")\n",
    "    \n",
    "    # Create mock data for demonstration\n",
    "    mock_images = torch.randn(100, 3, 224, 224)\n",
    "    mock_labels = torch.randint(0, 2, (100,))\n",
    "    \n",
    "    train_data = [(mock_images[i], mock_labels[i]) for i in range(80)]\n",
    "    test_data = [(mock_images[i], mock_labels[i]) for i in range(80, 100)]\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(\"Using mock data for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition (Transfer Learning with ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID19Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(COVID19Classifier, self).__init__()\n",
    "        \n",
    "        # Use ResNet18 as backbone\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze early layers for transfer learning\n",
    "        for param in list(self.resnet.parameters())[:-10]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize model\n",
    "model = COVID19Classifier().to(device)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return total_loss / len(test_loader), accuracy, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc, _, _ = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    print()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Final evaluation\n",
    "final_test_loss, final_test_acc, predictions, targets = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'COVID'], \n",
    "            yticklabels=['Normal', 'COVID'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(targets, predictions, target_names=['Normal', 'COVID']))\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "specificity = tn / (tn + fp)  # True Negative Rate\n",
    "\n",
    "print(f\"\\nSensitivity (COVID Detection): {sensitivity:.3f}\")\n",
    "print(f\"Specificity (Normal Detection): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': final_test_acc,\n",
    "    'epoch': num_epochs\n",
    "}, 'models/covid_classifier.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reflection and Learning Summary\n",
    "\n",
    "### Key Learnings from This COVID-19 Classification Task:\n",
    "\n",
    "1. **Data Quality Matters**: Medical imaging datasets require careful preprocessing and validation - the quality and representativeness of chest X-ray images significantly impacts model performance, and class imbalance is a common challenge in medical datasets.\n",
    "\n",
    "2. **Transfer Learning Effectiveness**: Using pre-trained ResNet models on ImageNet provides excellent feature extraction capabilities for medical images, even though they weren't originally trained on medical data - this demonstrates the power of learned visual representations.\n",
    "\n",
    "3. **Medical AI Limitations**: Achieving high accuracy on limited datasets doesn't guarantee clinical utility - real-world deployment requires extensive validation, diverse patient populations, and consideration of edge cases that may not be present in research datasets.\n",
    "\n",
    "4. **Evaluation Metrics Importance**: In medical applications, sensitivity (detecting true COVID cases) and specificity (avoiding false positives) are often more important than overall accuracy - a model with 90% accuracy but poor sensitivity could miss critical cases.\n",
    "\n",
    "5. **Ethical Considerations**: COVID-19 classification models highlight the responsibility of AI developers in healthcare - false negatives could delay treatment while false positives could cause unnecessary panic, emphasizing the need for careful validation and human oversight.\n",
    "\n",
    "6. **Data Augmentation Benefits**: Simple augmentation techniques like rotation and flipping help improve model robustness without requiring additional data collection, which is particularly valuable in medical imaging where acquiring new labeled data is expensive and time-consuming.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}